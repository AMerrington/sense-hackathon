{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "SENSE CDT Practical Session - Template.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AMerrington/sense-hackathon/blob/alex/SENSE_CDT_Practical_Session_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHC21YQ7CIHL"
      },
      "source": [
        "# Automated Sentinel-1 Ice, Water, Land Segmentation Challenge\n",
        "\n",
        "\n",
        "\n",
        "This notebook is intended as a template only, to help guide through the training process. Feel free to use as little or as much of it as you like.\n",
        "\n",
        "For the purposes of the template, we will assume a *classification* approach, which involves sub-sampling small images from the Sentinel-1 images. There will be notes where code should be adjusted for a *segmentation* approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OKRmGMNPPwS"
      },
      "source": [
        "### Mounting Google Drive\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbXyq4e_PORL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4a697b5-b88f-4fa5-b84e-7797bc071751"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "\r\n",
        "# Change the directory to the google drive\r\n",
        "%cd /content/drive/My\\ Drive/\r\n",
        "\r\n",
        "import sys\r\n",
        "sys.path.append('/content/drive/MyDrive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6b53UiBB8Kh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "075125dc-438e-4424-b2a3-b3e03a6b0ac4"
      },
      "source": [
        "!pip install xarray\r\n",
        "!pip install rasterio\r\n",
        "!pip install geopandas"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xarray in /usr/local/lib/python3.7/dist-packages (0.15.1)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.7/dist-packages (from xarray) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from xarray) (1.19.5)\n",
            "Requirement already satisfied: setuptools>=41.2 in /usr/local/lib/python3.7/dist-packages (from xarray) (53.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25->xarray) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25->xarray) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.25->xarray) (1.15.0)\n",
            "Collecting rasterio\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cb/ed/aa7cc593dbcb974f80ca0a15967d44abc820dbeb063e01478c95adcca156/rasterio-1.2.0-cp37-cp37m-manylinux1_x86_64.whl (19.1MB)\n",
            "\u001b[K     |████████████████████████████████| 19.1MB 45.1MB/s \n",
            "\u001b[?25hCollecting snuggs>=1.4.1\n",
            "  Downloading https://files.pythonhosted.org/packages/cc/0e/d27d6e806d6c0d1a2cfdc5d1f088e42339a0a54a09c3343f7f81ec8947ea/snuggs-1.4.7-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rasterio) (1.19.5)\n",
            "Collecting click-plugins\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/da/824b92d9942f4e472702488857914bdd50f73021efea15b4cad9aca8ecef/click_plugins-1.1.1-py2.py3-none-any.whl\n",
            "Collecting cligj>=0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/42/1e/947eadf10d6804bf276eb8a038bd5307996dceaaa41cfd21b7a15ec62f5d/cligj-0.7.1-py3-none-any.whl\n",
            "Requirement already satisfied: click<8,>=4.0 in /usr/local/lib/python3.7/dist-packages (from rasterio) (7.1.2)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from rasterio) (20.3.0)\n",
            "Collecting affine\n",
            "  Downloading https://files.pythonhosted.org/packages/ac/a6/1a39a1ede71210e3ddaf623982b06ecfc5c5c03741ae659073159184cd3e/affine-2.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from rasterio) (2020.12.5)\n",
            "Requirement already satisfied: pyparsing>=2.1.6 in /usr/local/lib/python3.7/dist-packages (from snuggs>=1.4.1->rasterio) (2.4.7)\n",
            "Installing collected packages: snuggs, click-plugins, cligj, affine, rasterio\n",
            "Successfully installed affine-2.3.0 click-plugins-1.1.1 cligj-0.7.1 rasterio-1.2.0 snuggs-1.4.7\n",
            "Collecting geopandas\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2a/9f/e8a440a993e024c0d3d4e5c7d3346367c50c9a1a3d735caf5ee3bde0aab1/geopandas-0.8.2-py2.py3-none-any.whl (962kB)\n",
            "\u001b[K     |████████████████████████████████| 972kB 11.9MB/s \n",
            "\u001b[?25hCollecting pyproj>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/0c/d7c2c7c370ea5368b813a44e772247ed1a461dc47de70c5d02e079abc7e0/pyproj-3.0.0.post1-cp37-cp37m-manylinux2010_x86_64.whl (6.4MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5MB 33.3MB/s \n",
            "\u001b[?25hCollecting fiona\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/47/c2/67d1d0acbaaee3b03e5e22e3b96c33219cb5dd392531c9ff9cee7c2eb3e4/Fiona-1.8.18-cp37-cp37m-manylinux1_x86_64.whl (14.8MB)\n",
            "\u001b[K     |████████████████████████████████| 14.8MB 305kB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from geopandas) (1.1.5)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.7/dist-packages (from geopandas) (1.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from pyproj>=2.2.0->geopandas) (2020.12.5)\n",
            "Collecting munch\n",
            "  Downloading https://files.pythonhosted.org/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.7/dist-packages (from fiona->geopandas) (0.7.1)\n",
            "Requirement already satisfied: attrs>=17 in /usr/local/lib/python3.7/dist-packages (from fiona->geopandas) (20.3.0)\n",
            "Requirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.7/dist-packages (from fiona->geopandas) (1.1.1)\n",
            "Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.7/dist-packages (from fiona->geopandas) (1.15.0)\n",
            "Requirement already satisfied: click<8,>=4.0 in /usr/local/lib/python3.7/dist-packages (from fiona->geopandas) (7.1.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.0->geopandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.0->geopandas) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.0->geopandas) (2.8.1)\n",
            "Installing collected packages: pyproj, munch, fiona, geopandas\n",
            "Successfully installed fiona-1.8.18 geopandas-0.8.2 munch-2.5.0 pyproj-3.0.0.post1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRB1-YPwCIHo"
      },
      "source": [
        "### Dataset preparation - (1) sub-sampling\n",
        "\n",
        "Sample patches from each TIF image, and find the corresponding label using the Shapefiles. Save each image with a unique ID save in the directory **SAMPLING_DIR**. Save the corresponding meta data in the following format (this could be a CSV file, NumPy array, or some other format), in the directory **META_DIR**:\n",
        "\n",
        "\n",
        "```\n",
        "image_id, x, y, label\n",
        "```\n",
        "\n",
        "Set the label value as one of \"L\", \"W\", \"I\" as specified in the Shapefiles.\n",
        "\n",
        "To make it easier to patch the final segmentation back together, it is suggested to use the (x, y) pixel coordinates of the patch, rather than the spatial coordinates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "manFwzp040Bn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d48d859-b635-414b-95bf-ee357e6d919f"
      },
      "source": [
        "tile.values.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(152, 155, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "bUXbxiWIncYU",
        "outputId": "711e6bad-f1eb-45a7-e4df-cb3b54760609"
      },
      "source": [
        "\"\"\"\r\n",
        "Importing the S1 images\r\n",
        "\"\"\"\r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "import xarray as xr\r\n",
        "from write_tiff_funcs_DTM import create_geoTrans\r\n",
        "from write_tiff_funcs_DTM import check_array_orientation\r\n",
        "from write_tiff_funcs_DTM import write_xarray_to_GeoTiff\r\n",
        "\r\n",
        "# Setting directories\r\n",
        "SAMPLING_DIR = \"/content/drive/MyDrive/Sentinel geotiffs/tiff_samples\"\r\n",
        "META_DIR = \"/content/drive/MyDrive/Sentinel geotiffs/tiff_sample_metadata\"\r\n",
        "S1_tiffs= \"/content/drive/MyDrive/Sentinel geotiffs\"\r\n",
        "\r\n",
        "# Importing S1 tiffs.\r\n",
        "for filename in os.listdir(S1_tiffs):\r\n",
        "    # Opening tiffs as an xarray.\r\n",
        "    raster= xr.open_rasterio('%s/%s' % (S1_tiffs, filename))\r\n",
        "    n_bands= raster.values.shape[0]\r\n",
        "    \r\n",
        "    # THIS IS NOT REFLECTANCE but does rescale (normalise bands) from 0 to 1.\r\n",
        "    #img= exposure.rescale_intensity(raster)\r\n",
        "    \r\n",
        "    # apply NaN values to empty cells in multispec image.\r\n",
        "    for ii in range(n_bands):\r\n",
        "        raster.values= raster.values.astype('float')\r\n",
        "        raster.values[raster.values==raster.nodatavals[ii]]= np.nan\r\n",
        "        #raster= raster.rio.reproject('EPSG:27700') # reprojecting to British National Grid for use in GIS.\r\n",
        "    \r\n",
        "    raster= raster.transpose('y','x','band') \r\n",
        "\r\n",
        "    # Tiling this S1 tiff.\r\n",
        "    img_shape = raster.values.shape\r\n",
        "    num_tiles= 1000\r\n",
        "    x_size= img_shape[1]/num_tiles\r\n",
        "    y_size= img_shape[0]/num_tiles\r\n",
        "\r\n",
        "    # xarray\r\n",
        "    j= (0, 0)\r\n",
        "    offset = (int(y_size), int(x_size))\r\n",
        "    segments= np.zeros(raster.values.shape[0:2])\r\n",
        "    \r\n",
        "    x_coords= raster.coords['x'].values\r\n",
        "    y_coords= raster.coords['y'].values\r\n",
        "\r\n",
        "    for i in range(1, num_tiles + 1): \r\n",
        "            for ii in range(1, num_tiles + 1): \r\n",
        "                tile = raster.sel(x=slice(x_coords[j[0]], x_coords[offset[1]*i-1]), y=slice(y_coords[j[1]], y_coords[offset[0]*ii-1]))\r\n",
        "                # Writing out the tiled tiff\r\n",
        "                geoTrans= create_geoTrans(tile, x_name= 'x', y_name= 'y')\r\n",
        "                check_array_orientation(tile,geoTrans,north_up=True)\r\n",
        "                write_xarray_to_GeoTiff(tile, SAMPLING_DIR, north_up=True)\r\n",
        "                # Moving the offset to the next column (row keeps static).\r\n",
        "                j= (j[0], offset[1]*ii)\r\n",
        "            # Moving to the next row and resetting the column offsets.\r\n",
        "            j= (offset[0]*i, 0)\r\n",
        "            print('Tile segmented: '+ str(i) +' of '+ str(num_tiles))\r\n",
        "\r\n",
        "    \r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-65323e53c7a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mgeoTrans\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcreate_geoTrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_name\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_name\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mcheck_array_orientation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgeoTrans\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnorth_up\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0mwrite_xarray_to_GeoTiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSAMPLING_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorth_up\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m                 \u001b[0;31m# Moving the offset to the next column (row keeps static).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mj\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/write_tiff_funcs_DTM.py\u001b[0m in \u001b[0;36mwrite_xarray_to_GeoTiff\u001b[0;34m(array, outfilename, north_up)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;31m# write array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetRasterBand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSetNoDataValue\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m9999\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetRasterBand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWriteArray\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/osgeo/gdal.py\u001b[0m in \u001b[0;36mWriteArray\u001b[0;34m(self, array, xoff, yoff, resample_alg, callback, callback_data)\u001b[0m\n\u001b[1;32m   2552\u001b[0m                                            \u001b[0mresample_alg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresample_alg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m                                            \u001b[0mcallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2554\u001b[0;31m                                            callback_data = callback_data )\n\u001b[0m\u001b[1;32m   2555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2556\u001b[0m     def GetVirtualMemArray(self, eAccess = gdalconst.GF_Read, xoff=0, yoff=0,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/osgeo/gdal_array.py\u001b[0m in \u001b[0;36mBandWriteArray\u001b[0;34m(band, array, xoff, yoff, resample_alg, callback, callback_data)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"expected array of dim 2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0mxsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: expected array of dim 2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "6YRj47QkCIHu"
      },
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhWosYr8CIHz"
      },
      "source": [
        "Some helpful code: reading in a single Sentinel-1 image and the corresponding Shapefile."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "bnhFqDEWCIH3"
      },
      "source": [
        "# the directory containing all shapefiles - i.e., the location of sea_ice/ \n",
        "SHAPEFILE_DIR = \"/content/drive/MyDrive/EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice\" \n",
        "\n",
        "\n",
        "shapefile = SHAPEFILE_DIR + \"\" # full name of .shp file\n",
        "\n",
        "# extract the shape ID, for example, 20180116T075430\n",
        "shp_id = shapefile.split(\"_\")[-1][:-4].upper()\n",
        "\n",
        "# locate the corresponding Sentinel-1 image based on the ID\n",
        "# this should only return 1 match, which you can confirm\n",
        "tiff_file = [g for g in tiff_files if shp_id in g]\n",
        "tiff_file = tiff_file[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoD945ZDCIH7"
      },
      "source": [
        "Feel free to use other Python packages; but as an example, here we use **geopandas** to read in the Shapefile, and **rasterio** to read the GeoTIFF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "XHn9oUhxCIH-"
      },
      "source": [
        "shape_data = gpd.read_file(SHAPEFILE_DIR + shapefile)\n",
        "\n",
        "shape_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "o0SCTP7nCIIA"
      },
      "source": [
        "# directory containing all GeoTIFF files\n",
        "TIFF_DIR = \"\"\n",
        "\n",
        "tif_img = rasterio.open(TIFF_DIR + tiff_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "I5UEcPgICIIE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "0qlcg9KCCIIG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O4r1NNECIIK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 843
        },
        "outputId": "f12ca6a6-d0ba-44f2-a319-9b1f2e9b267c"
      },
      "source": [
        "import os\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import geopandas as gpd\r\n",
        "import cv2\r\n",
        "import rasterio\r\n",
        "from shapely.geometry import Point\r\n",
        "from os import listdir\r\n",
        "from os.path import isfile, join\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "# Setting directories\r\n",
        "SAMPLING_DIR = \"/content/drive/MyDrive/Sentinel geotiffs/png_samples_100x100/\"\r\n",
        "META_DIR = \"/content/drive/MyDrive/Sentinel geotiffs/tiff_sample_metadata/\"\r\n",
        "TIFF_DIR= \"/content/drive/MyDrive/Sentinel geotiffs/\"\r\n",
        "SHAPEFILE_DIR= \"/content/drive/MyDrive/EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/\"\r\n",
        "\r\n",
        "# Set up training arguments and parse\r\n",
        "max_tries= 1200\r\n",
        "samples_per_label= 400\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# one by one read in Shapefiles\r\n",
        "shp_files = [f for f in listdir(SHAPEFILE_DIR) if isfile(join(SHAPEFILE_DIR, f)) and \".shp\" in f]\r\n",
        "tiff_files = [f for f in listdir(TIFF_DIR) if isfile(join(TIFF_DIR, f)) and \".tif\" in f]\r\n",
        "\r\n",
        "print(\"Shapefiles found:       \", len(shp_files))\r\n",
        "print(\"GeoTIFF files found:    \", len(tiff_files))\r\n",
        "\r\n",
        "# function to determine whether a spatial coordinate is in any of the listed shapes\r\n",
        "def get_label(shape_data, spatial_coords):\r\n",
        "\r\n",
        "\tspa_x, spa_y = spatial_coords\r\n",
        "\r\n",
        "\tfor i, poly in enumerate(shape_data['geometry']):\r\n",
        "\t\tif poly.contains(Point(spa_x, spa_y)):\r\n",
        "\t\t\treturn shape_data['poly_type'][i]\r\n",
        "\r\n",
        "\treturn None\r\n",
        "\r\n",
        "\r\n",
        "# each row corresponds to a sub_image - image ix, (x, y) for top left corner, label\r\n",
        "meta = []\r\n",
        "\r\n",
        "idx = 7520  # unique sub-sample image idx\r\n",
        "\r\n",
        "def is_quota_met(count_dict):\r\n",
        "\tfor val in count_dict.values():\r\n",
        "\t\tif val < samples_per_label:\r\n",
        "\t\t\treturn False\r\n",
        "\treturn True\r\n",
        "\r\n",
        "\r\n",
        "# Local testing constraint\r\n",
        "# shp_files = shp_files[:2]\r\n",
        "# K defines the size of the image in pixels (KxK).\r\n",
        "K= 100\r\n",
        "for shpfile in shp_files:\r\n",
        "\ttries = 0\r\n",
        "\tshp_id = shpfile.split(\"_\")[-1][:-4].upper()\r\n",
        "\tshape_data = gpd.read_file(SHAPEFILE_DIR + shpfile)\r\n",
        "\r\n",
        "\t# reset the sample count dict per image\r\n",
        "\tsample_count = {\r\n",
        "\t\t\"L\": 0,\r\n",
        "\t\t\"W\": 0,\r\n",
        "\t\t\"I\": 0,\r\n",
        "\t}\r\n",
        "\r\n",
        "\t# read in associated GeoTIFF file\r\n",
        "\ttiff_file = [g for g in tiff_files if shp_id in g]\r\n",
        "\tprint(tiff_file[0])\r\n",
        "  \r\n",
        "\tif len(tiff_file):\r\n",
        "\t\tsrc = rasterio.open(TIFF_DIR + tiff_file[0])\r\n",
        "\r\n",
        "\t\t# get numpy version for sampling sub_image\r\n",
        "\t\tnp_tiff = np.rollaxis(src.read(), 0, 3)\r\n",
        "    \r\n",
        "\t\twhile not is_quota_met(sample_count) and tries < max_tries:\r\n",
        "\r\n",
        "\t\t\ttries += 1\r\n",
        "\r\n",
        "\t\t\t# select random x, y in src.width, src.height\r\n",
        "\t\t\tx_pos = np.random.randint(0, src.width)\r\n",
        "\t\t\ty_pos = np.random.randint(0, src.height)\r\n",
        "      \r\n",
        "\t\t\tspatial_coords = src.transform * (x_pos + int(K/2), y_pos + int(K/2))\r\n",
        "\r\n",
        "\t\t\t# is this position in a shape?\r\n",
        "\t\t\tlabel = get_label(shape_data, spatial_coords)\r\n",
        "\t\t\tif label and sample_count[label] < samples_per_label:\r\n",
        "\t\t\t\tsub_im = np_tiff[x_pos:(x_pos + K), y_pos:(y_pos + K)]\r\n",
        "\r\n",
        "\t\t\t\tif sub_im.shape == (K, K, 3):\r\n",
        "\t\t\t\t\tcv2.imwrite(SAMPLING_DIR + str(idx) + \".png\", sub_im)\r\n",
        "\t\t\t\t\tmeta.append([idx, x_pos, y_pos, label])\r\n",
        "\t\t\t\t\tidx += 1\r\n",
        "\r\n",
        "\t\t\t\t\tsample_count[label] += 1\r\n",
        "\r\n",
        "\tprint(\"Number of tries:\", tries)\r\n",
        "\r\n",
        "print(\"\")\r\n",
        "print(\"Final idx:              \", idx)\r\n",
        "print(\"Meta length:            \", len(meta))\r\n",
        "np.save(META_DIR + \"meta3.npy\", np.array(meta))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shapefiles found:        12\n",
            "GeoTIFF files found:     25\n",
            "S1B_EW_GRDM_1SDH_20180515T174633_20180515T174733_010935_01403A_A84D_Orb_Cal_Spk_TC_rgb_8bit.tif\n",
            "Number of tries: 1200\n",
            "S1A_EW_GRDM_1SDH_20180717T073809_20180717T073909_022831_0279B9_EBF1_Orb_Cal_Spk_TC_rgb_8bit.tif\n",
            "Number of tries: 1200\n",
            "S1B_EW_GRDM_1SDH_20181113T074529_20181113T074629_013583_019254_D382_Orb_Cal_Spk_TC_rgb_8bit.tif\n",
            "Number of tries: 1200\n",
            "S1B_EW_GRDM_1SDH_20180213T175444_20180213T175544_009608_011511_8266_Orb_Cal_Spk_TC_rgb_8bit.tif\n",
            "Number of tries: 1200\n",
            "S1A_EW_GRDM_1SDH_20180612T180423_20180612T180523_022327_026AB3_AC33_Orb_Cal_Spk_TC_rgb_8bit.tif\n",
            "Number of tries: 1200\n",
            "S1A_EW_GRDM_1SDH_20180116T075430_20180116T075530_020177_0226B9_9FE3_Orb_Cal_Spk_TC_rgb_8bit.tif\n",
            "Number of tries: 1200\n",
            "S1A_EW_GRDM_1SDH_20180313T181225_20180313T181325_021000_0240E1_8163_Orb_Cal_Spk_TC_rgb_8bit.tif\n",
            "Number of tries: 1200\n",
            "S1A_EW_GRDM_1SDH_20180417T074606_20180417T074706_021504_0250C3_D211_Orb_Cal_Spk_TC_rgb_8bit.tif\n",
            "Number of tries: 1200\n",
            "S1A_EW_GRDM_1SDH_20181218T075437_20181218T075537_025077_02C472_1DB2_Orb_Cal_Spk_TC_rgb_8bit.tif\n",
            "Number of tries: 1200\n",
            "S1A_EW_GRDM_1SDH_20180911T175548_20180911T175652_023654_0293F5_7CA2_Orb_Cal_Spk_TC_rgb_8bit.tif\n",
            "Number of tries: 1200\n",
            "S1A_EW_GRDM_1SDH_20181016T072958_20181016T073058_024158_02A460_DA8F_Orb_Cal_Spk_TC_rgb_8bit.tif\n",
            "Number of tries: 1200\n",
            "S1B_EW_GRDM_1SDH_20180814T075344_20180814T075444_012256_016952_B1DC_Orb_Cal_Spk_TC_rgb_8bit.tif\n",
            "Number of tries: 1200\n",
            "\n",
            "Final idx:               13741\n",
            "Meta length:             6221\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-0d570d2b6eee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Final idx:              \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Meta length:            \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMETA_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"meta3.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msave\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m         \u001b[0mfile_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_ctx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Sentinel geotiffs/tiff_sample_metadata/meta3.npy'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khil-wzxCIIQ"
      },
      "source": [
        "The shapes in the Shapefiles are **shapely** objects. We can also use the Python package **shapely** to check whether an x, y pixel coordinate position is in a given polyshape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "8YwH-36UCIIR"
      },
      "source": [
        "from shapely.geometry import Point\n",
        "\n",
        "x = 4000\n",
        "y = 8000\n",
        "\n",
        "point = Point(x, y)\n",
        "\n",
        "# for example, specify the shape in the Shapefile\n",
        "shape_id = 2\n",
        "\n",
        "if shape_data['geometry'][shape_id].contains(point):\n",
        "    print(\"Point\", point, \"is in shape\", shape_id, \"and has class\", shape_data['poly_type'][shape_id])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "fUtcOjw_CIIU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc7a3736-498b-4372-efa4-e7150f0ebf2c"
      },
      "source": [
        "x=np.load('/content/drive/MyDrive/Sentinel geotiffs/tiff_sample_metadata/meta3.npy')\r\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['7520', '10906', '6314', 'W'],\n",
              "       ['7521', '9200', '8760', 'W'],\n",
              "       ['7522', '9828', '2899', 'W'],\n",
              "       ...,\n",
              "       ['10015', '3615', '7571', 'I'],\n",
              "       ['10016', '2659', '5567', 'L'],\n",
              "       ['10017', '2205', '6378', 'L']], dtype='<U21')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5LmKwtwCIIV"
      },
      "source": [
        "Define a train/validation ratio. Patches and meta saved from the test TIF images should be stored in separate directories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "PMhwWSsmCIIW"
      },
      "source": [
        "TRAIN_SIZE = 0.7\n",
        "\n",
        "# valid size = 1.0 - TRAIN_SIZE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRvR5z1ICIIY"
      },
      "source": [
        "Map the class category characters to integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "GM6RCG7qCIIZ"
      },
      "source": [
        "LABELS = {\n",
        "\t\"L\": 0,\n",
        "\t\"W\": 1,\n",
        "\t\"I\": 2,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLo2vY_aCIIa"
      },
      "source": [
        "The following is a Dataset class which reads in image data saved in the format described above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "3i9scAOyCIIb"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class PolarPatch(Dataset):\n",
        "    def __init__(self, transform=None, split=\"train\"):\n",
        "        super(PolarPatch, self).__init__()\n",
        "\n",
        "        assert split in [\"train\", \"val\"]\n",
        "        \n",
        "        # TODO: load in meta data, which should be of shape (3, N) - N being the number of samples\n",
        "        meta = []\n",
        "\n",
        "        train_dim = int(TRAIN_SIZE * len(meta))\n",
        "        \n",
        "        if split == \"train\":\n",
        "            meta = meta[:train_dim]\n",
        "        else:\n",
        "            meta = meta[train_dim:]                   \n",
        "\n",
        "        self.images = range(len(meta))\n",
        "        self.coords = [(row[1], row[2]) for row in meta]\n",
        "\n",
        "        # Targets in integer form for computing cross entropy\n",
        "        self.targets = [LABELS[row[3]] for row in meta]\n",
        "        self.transform = transform\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        x = Image.open(SAMPLING_DIR + str(self.images[index]) + \".png\") # change this file format if needed\n",
        "        y = self.targets[index]\n",
        "        coord = self.coords[index]\n",
        "\n",
        "        if self.transform:\n",
        "        \tx = self.transform(x)\n",
        "\n",
        "        return x, y, coord"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFTDoXnHCIIc"
      },
      "source": [
        "An example data transform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "E9eibJLVCIId"
      },
      "source": [
        "data_transform = transforms.Compose([\n",
        "    # TODO: add whatever else you need - normalisation, augmentation, etc.\n",
        "\ttransforms.ToTensor(),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytX8tcZNCIIe"
      },
      "source": [
        "### Dataset preparation - (2) data loaders\n",
        "\n",
        "Now we can prepare the data loaders. Here is the example for the training set; you will also need the validation and test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "gtPvchfgCIIf"
      },
      "source": [
        "import torch\n",
        "\n",
        "# TODO set this value based on your working environment\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "train_set = PolarPatch(\n",
        "    split='train',\n",
        "    transform=data_transform\n",
        ")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_set,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "reqHjIplCIIh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZHmGtWvCIIi"
      },
      "source": [
        "### Model\n",
        "\n",
        "You can use a custom model architecture, or copy one from literature. It is recommended to not build too deep of a network for the sake of training time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "DDFmxJQfCIIj"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class PolarNet(nn.Module):\n",
        "    def __init__(self, n_classes=3):\n",
        "        super(PolarNet, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            # TODO: build your own architecture here; one conv layer and ReLU here as an example only\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True), \n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            # TODO: continue classifier section of architecture here for classification approach;\n",
        "            # otherwise, remove and add in upscaling for a fully-convolutional segmentation approach \n",
        "            nn.Linear(4096, n_classes),\n",
        "        )      \n",
        "\n",
        "    def forward(self, x):\n",
        "        # as an example; alter as needed depending on your architecture\n",
        "        x = self.features(x)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "PLo6irWfCIIj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oR8lOEJ9CIIk"
      },
      "source": [
        "### Training\n",
        "\n",
        "An example of loading the model, setting a loss criteria and defining an optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "suIqXQXQCIIl"
      },
      "source": [
        "# Device configuration - defaults to CPU unless GPU is available on device\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "kFbfStvdCIIm"
      },
      "source": [
        "from torch import optim\n",
        "\n",
        "model = PolarNet().to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Stochastic gradient descent - TODO: alter as needed\n",
        "optimizer = optim.SGD(\n",
        "\tmodel.parameters(),\n",
        "\tlr=0.001,\n",
        "\tweight_decay=0.0005,\n",
        "\tmomentum=0.9,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpPQe9HyCIIn"
      },
      "source": [
        "Train the model, batch by batch, for as many iterations as required to converge. You can use the validation set to determine automatically when to stop training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "cLO_0V8-CIIo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "7wetrLtwCIIq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AGEPf4KCIIr"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "Evaluate patch-based accuracy on the test set; then using the test patch coordinates, piece together the segmentation prediction on the original TIF images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "9XB7DWCPCIIs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "uZgvi4HICIIt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "mhVJvSm2CIIu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}