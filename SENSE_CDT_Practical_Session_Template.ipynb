{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "SENSE CDT Practical Session - Template.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AMerrington/sense-hackathon/blob/alex/SENSE_CDT_Practical_Session_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHC21YQ7CIHL"
      },
      "source": [
        "# Automated Sentinel-1 Ice, Water, Land Segmentation Challenge\n",
        "\n",
        "\n",
        "\n",
        "This notebook is intended as a template only, to help guide through the training process. Feel free to use as little or as much of it as you like.\n",
        "\n",
        "For the purposes of the template, we will assume a *classification* approach, which involves sub-sampling small images from the Sentinel-1 images. There will be notes where code should be adjusted for a *segmentation* approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OKRmGMNPPwS"
      },
      "source": [
        "### Mounting Google Drive\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbXyq4e_PORL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e02309b1-89cc-4956-8734-8ee176dccaa2"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "\r\n",
        "# Change the directory to the google drive\r\n",
        "%cd /content/drive/My\\ Drive/\r\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRB1-YPwCIHo"
      },
      "source": [
        "### Dataset preparation - (1) sub-sampling\n",
        "\n",
        "Sample patches from each TIF image, and find the corresponding label using the Shapefiles. Save each image with a unique ID save in the directory **SAMPLING_DIR**. Save the corresponding meta data in the following format (this could be a CSV file, NumPy array, or some other format), in the directory **META_DIR**:\n",
        "\n",
        "\n",
        "```\n",
        "image_id, x, y, label\n",
        "```\n",
        "\n",
        "Set the label value as one of \"L\", \"W\", \"I\" as specified in the Shapefiles.\n",
        "\n",
        "To make it easier to patch the final segmentation back together, it is suggested to use the (x, y) pixel coordinates of the patch, rather than the spatial coordinates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUXbxiWIncYU"
      },
      "source": [
        "\"\"\"\r\n",
        "Importing the S1 images\r\n",
        "\"\"\"\r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "import xarray as xr\r\n",
        "from write_tiff_funcs_DTM import create_geoTrans\r\n",
        "from write_tiff_funcs_DTM import check_array_orientation\r\n",
        "from write_tiff_funcs_DTM import write_xarray_to_GeoTiff\r\n",
        "\r\n",
        "# Setting directories\r\n",
        "SAMPLING_DIR = \"/content/drive/MyDrive/Sentinel geotiffs/tiff_samples\"\r\n",
        "META_DIR = \"/content/drive/MyDrive/Sentinel geotiffs/tiff_sample_metadata\"\r\n",
        "S1_tiffs= \"/content/drive/MyDrive/Sentinel geotiffs\"\r\n",
        "\r\n",
        "# Importing S1 tiffs.\r\n",
        "for filename in os.listdir(S1_tiffs):\r\n",
        "    # Opening tiffs as an xarray.\r\n",
        "    raster= xr.open_rasterio('%s/%s' % (data_dir, raster_file))\r\n",
        "    n_bands= raster.values.shape[0]\r\n",
        "    \r\n",
        "    # THIS IS NOT REFLECTANCE but does rescale (normalise bands) from 0 to 1.\r\n",
        "    #img= exposure.rescale_intensity(raster)\r\n",
        "    \r\n",
        "    # apply NaN values to empty cells in multispec image.\r\n",
        "    for ii in range(n_bands):\r\n",
        "        raster.values= raster.values.astype('float')\r\n",
        "        raster.values[raster.values==raster.nodatavals[ii]]= np.nan\r\n",
        "        #raster= raster.rio.reproject('EPSG:27700') # reprojecting to British National Grid for use in GIS.\r\n",
        "    \r\n",
        "    raster= raster.transpose('y','x','band') \r\n",
        "\r\n",
        "    # Tiling this S1 tiff.\r\n",
        "    img_shape = raster.values.shape\r\n",
        "    num_tiles= 10\r\n",
        "    x_size= img_shape[1]/10\r\n",
        "    y_size= img_shape[0]/10\r\n",
        "\r\n",
        "    # xarray\r\n",
        "    j= (0, 0)\r\n",
        "    offset = (int(y_size), int(x_size))\r\n",
        "    segments= np.zeros(raster.values.shape[0:2])\r\n",
        "    \r\n",
        "    x_coords= raster.coords['x'].values\r\n",
        "    y_coords= raster.coords['y'].values\r\n",
        "\r\n",
        "    for i in range(1, num_tiles + 1): \r\n",
        "            for ii in range(1, num_tiles + 1): \r\n",
        "                tile = raster.sel(x=slice(x_coords[j[0]], x_coords[offset[1]*i-1]), y=slice(y_coords[j[1]], y_coords[offset[0]*ii-1]))\r\n",
        "                # Running the segmentation\r\n",
        "                segments[j[0]:offset[0]*i, j[1]:offset[1]*ii]= (felzenszwalb(tile, scale= scale, sigma= sigma, min_size= min_size)) + np.amax(segments) + 1\r\n",
        "                # Moving the offset to the next column (row keeps static).\r\n",
        "                j= (j[0], offset[1]*ii)\r\n",
        "            # Moving to the next row and resetting the column offsets.\r\n",
        "            j= (offset[0]*i, 0)\r\n",
        "\r\n",
        "            # Writing out the tiled tiff\r\n",
        "            # Creating xarray for the segmented array with the input geotiff crs/dimensions.\r\n",
        "            segmented = raster[:,:,0].copy()\r\n",
        "            segmented.values= segments\r\n",
        "            \r\n",
        "            # Functions for writing out segment data as a GeoTiff.\r\n",
        "            geoTrans= create_geoTrans(segmented, x_name= 'x', y_name= 'y')\r\n",
        "            check_array_orientation(segmented,geoTrans,north_up=True)\r\n",
        "            write_xarray_to_GeoTiff(segmented, 'OUTPUT_fn, north_up=True)\r\n",
        "            \r\n",
        "            print('Tile segmented: '+ str(i) +' of '+ str(num_tiles))\r\n",
        "\r\n",
        "    \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "6YRj47QkCIHu"
      },
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhWosYr8CIHz"
      },
      "source": [
        "Some helpful code: reading in a single Sentinel-1 image and the corresponding Shapefile."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "bnhFqDEWCIH3"
      },
      "source": [
        "# the directory containing all shapefiles - i.e., the location of sea_ice/ \n",
        "SHAPEFILE_DIR = \"/content/drive/MyDrive/EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice\" \n",
        "\n",
        "\n",
        "shapefile = SHAPEFILE_DIR + \"\" # full name of .shp file\n",
        "\n",
        "# extract the shape ID, for example, 20180116T075430\n",
        "shp_id = shapefile.split(\"_\")[-1][:-4].upper()\n",
        "\n",
        "# locate the corresponding Sentinel-1 image based on the ID\n",
        "# this should only return 1 match, which you can confirm\n",
        "tiff_file = [g for g in tiff_files if shp_id in g]\n",
        "tiff_file = tiff_file[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoD945ZDCIH7"
      },
      "source": [
        "Feel free to use other Python packages; but as an example, here we use **geopandas** to read in the Shapefile, and **rasterio** to read the GeoTIFF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "XHn9oUhxCIH-"
      },
      "source": [
        "shape_data = gpd.read_file(SHAPEFILE_DIR + shapefile)\n",
        "\n",
        "shape_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "o0SCTP7nCIIA"
      },
      "source": [
        "# directory containing all GeoTIFF files\n",
        "TIFF_DIR = \"\"\n",
        "\n",
        "tif_img = rasterio.open(TIFF_DIR + tiff_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "I5UEcPgICIIE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "0qlcg9KCCIIG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "9O4r1NNECIIK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khil-wzxCIIQ"
      },
      "source": [
        "The shapes in the Shapefiles are **shapely** objects. We can also use the Python package **shapely** to check whether an x, y pixel coordinate position is in a given polyshape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "8YwH-36UCIIR"
      },
      "source": [
        "from shapely.geometry import Point\n",
        "\n",
        "x = 4000\n",
        "y = 8000\n",
        "\n",
        "point = Point(x, y)\n",
        "\n",
        "# for example, specify the shape in the Shapefile\n",
        "shape_id = 2\n",
        "\n",
        "if shape_data['geometry'][shape_id].contains(point):\n",
        "    print(\"Point\", point, \"is in shape\", shape_id, \"and has class\", shape_data['poly_type'][shape_id])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Jj1r38qbCIIT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "fUtcOjw_CIIU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5LmKwtwCIIV"
      },
      "source": [
        "Define a train/validation ratio. Patches and meta saved from the test TIF images should be stored in separate directories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "PMhwWSsmCIIW"
      },
      "source": [
        "TRAIN_SIZE = 0.7\n",
        "\n",
        "# valid size = 1.0 - TRAIN_SIZE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRvR5z1ICIIY"
      },
      "source": [
        "Map the class category characters to integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "GM6RCG7qCIIZ"
      },
      "source": [
        "LABELS = {\n",
        "\t\"L\": 0,\n",
        "\t\"W\": 1,\n",
        "\t\"I\": 2,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLo2vY_aCIIa"
      },
      "source": [
        "The following is a Dataset class which reads in image data saved in the format described above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "3i9scAOyCIIb"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class PolarPatch(Dataset):\n",
        "    def __init__(self, transform=None, split=\"train\"):\n",
        "        super(PolarPatch, self).__init__()\n",
        "\n",
        "        assert split in [\"train\", \"val\"]\n",
        "        \n",
        "        # TODO: load in meta data, which should be of shape (3, N) - N being the number of samples\n",
        "        meta = []\n",
        "\n",
        "        train_dim = int(TRAIN_SIZE * len(meta))\n",
        "        \n",
        "        if split == \"train\":\n",
        "            meta = meta[:train_dim]\n",
        "        else:\n",
        "            meta = meta[train_dim:]                   \n",
        "\n",
        "        self.images = range(len(meta))\n",
        "        self.coords = [(row[1], row[2]) for row in meta]\n",
        "\n",
        "        # Targets in integer form for computing cross entropy\n",
        "        self.targets = [LABELS[row[3]] for row in meta]\n",
        "        self.transform = transform\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        x = Image.open(SAMPLING_DIR + str(self.images[index]) + \".png\") # change this file format if needed\n",
        "        y = self.targets[index]\n",
        "        coord = self.coords[index]\n",
        "\n",
        "        if self.transform:\n",
        "        \tx = self.transform(x)\n",
        "\n",
        "        return x, y, coord"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFTDoXnHCIIc"
      },
      "source": [
        "An example data transform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "E9eibJLVCIId"
      },
      "source": [
        "data_transform = transforms.Compose([\n",
        "    # TODO: add whatever else you need - normalisation, augmentation, etc.\n",
        "\ttransforms.ToTensor(),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytX8tcZNCIIe"
      },
      "source": [
        "### Dataset preparation - (2) data loaders\n",
        "\n",
        "Now we can prepare the data loaders. Here is the example for the training set; you will also need the validation and test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "gtPvchfgCIIf"
      },
      "source": [
        "import torch\n",
        "\n",
        "# TODO set this value based on your working environment\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "train_set = PolarPatch(\n",
        "    split='train',\n",
        "    transform=data_transform\n",
        ")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_set,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "reqHjIplCIIh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZHmGtWvCIIi"
      },
      "source": [
        "### Model\n",
        "\n",
        "You can use a custom model architecture, or copy one from literature. It is recommended to not build too deep of a network for the sake of training time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "DDFmxJQfCIIj"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class PolarNet(nn.Module):\n",
        "    def __init__(self, n_classes=3):\n",
        "        super(PolarNet, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            # TODO: build your own architecture here; one conv layer and ReLU here as an example only\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True), \n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            # TODO: continue classifier section of architecture here for classification approach;\n",
        "            # otherwise, remove and add in upscaling for a fully-convolutional segmentation approach \n",
        "            nn.Linear(4096, n_classes),\n",
        "        )      \n",
        "\n",
        "    def forward(self, x):\n",
        "        # as an example; alter as needed depending on your architecture\n",
        "        x = self.features(x)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "PLo6irWfCIIj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oR8lOEJ9CIIk"
      },
      "source": [
        "### Training\n",
        "\n",
        "An example of loading the model, setting a loss criteria and defining an optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "suIqXQXQCIIl"
      },
      "source": [
        "# Device configuration - defaults to CPU unless GPU is available on device\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "kFbfStvdCIIm"
      },
      "source": [
        "from torch import optim\n",
        "\n",
        "model = PolarNet().to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Stochastic gradient descent - TODO: alter as needed\n",
        "optimizer = optim.SGD(\n",
        "\tmodel.parameters(),\n",
        "\tlr=0.001,\n",
        "\tweight_decay=0.0005,\n",
        "\tmomentum=0.9,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpPQe9HyCIIn"
      },
      "source": [
        "Train the model, batch by batch, for as many iterations as required to converge. You can use the validation set to determine automatically when to stop training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "cLO_0V8-CIIo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "7wetrLtwCIIq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AGEPf4KCIIr"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "Evaluate patch-based accuracy on the test set; then using the test patch coordinates, piece together the segmentation prediction on the original TIF images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "9XB7DWCPCIIs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "uZgvi4HICIIt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "mhVJvSm2CIIu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}